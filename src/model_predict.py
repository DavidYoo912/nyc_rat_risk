# model_predict.py
import pandas as pd
import numpy as np
import os
import time
import pickle
import warnings
import folium
from folium.plugins import HeatMap
# Removed from IPython.display import display as it is not used in a production environment

# --- Path Setup to handle execution from inside 'src' ---
# Get the absolute path to the directory containing this script (e.g., /path/to/project/src)
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
# The project root is one directory up (e.g., /path/to/project)
PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)

# --- Configuration (Must match other scripts) ---
# Use absolute path construction relative to PROJECT_ROOT
ARTIFACTS_DIR_ROOT = os.path.join(PROJECT_ROOT, 'artifacts')
DATA_DIR = os.path.join(PROJECT_ROOT, 'data') # New: Directory for feature data
MODEL_DIR = os.path.join(ARTIFACTS_DIR_ROOT, 'models')
REPORT_DIR = os.path.join(ARTIFACTS_DIR_ROOT, 'reports')
# Ensure REPORT_DIR exists before saving reports
os.makedirs(REPORT_DIR, exist_ok=True) 

FEATURES_FILE = 'df_features_final.csv'
FEATURES = ['lag_1_rat', 'lag_1_dumping', 'lag_2_rat', 'lag_6_rat', 'lag_3_avg_rat', 'month', 'year']
COUNT_FEATURES = ['lag_1_rat', 'lag_1_dumping', 'lag_2_rat', 'lag_6_rat', 'lag_3_avg_rat']
TOP_N_MARKERS = 200 # Number of high-risk blocks to explicitly mark on the map

# --- WARNING SUPPRESSION BLOCK ---
# Temporarily ignore specific warnings to keep the output clean.
warnings.filterwarnings("ignore", category=UserWarning, message="X does not have valid feature names")
warnings.filterwarnings("ignore", category=UserWarning, message="The `max_val` parameter is no longer necessary.")
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=DeprecationWarning)
try:
    from pandas.errors import SettingWithCopyWarning
    warnings.filterwarnings("ignore", category=SettingWithCopyWarning)
except ImportError:
    pass
# ---------------------------------


def load_deployment_artifacts():
    """Loads necessary data and artifacts for live prediction."""
    print("--- 1. Loading Deployment Artifacts (Model, Scaler, Data) ---")
    
    artifacts = {}
    try:
        # Load Model (from models subdirectory)
        model_path = os.path.join(MODEL_DIR, 'best_model.pkl')
        with open(model_path, 'rb') as f:
            artifacts['model'] = pickle.load(f)
            
        # Load Scaler (from models subdirectory)
        scaler_path = os.path.join(MODEL_DIR, 'scaler.pkl')
        with open(scaler_path, 'rb') as f:
            artifacts['scaler'] = pickle.load(f)
        
        # Load all features data generated by data_pipeline.py (from the data directory)
        data_path = os.path.join(DATA_DIR, FEATURES_FILE)
        df_features = pd.read_csv(data_path)
        
        # Ensure no NaN Block_IDs remain (Crucial Fix)
        df_features.dropna(subset=['Block_ID'], inplace=True)
        artifacts['df_features'] = df_features
            
    except FileNotFoundError as e:
        # Adjusted error message to reflect the new data path check
        print(f"Error: Required artifact not found: {e}.")
        print(f"Check MODEL_DIR: {MODEL_DIR} and DATA_DIR: {DATA_DIR}.")
        print("Ensure prior scripts were run successfully and files are in the correct subdirectories.")
        return None
    except Exception as e:
        print(f"An unexpected error occurred during artifact loading: {e}")
        return None
    
    print("Artifacts loaded successfully.")
    return artifacts


def generate_proactive_predictions(artifacts):
    """
    Identifies the latest month, scales data, generates predictions, 
    and filters the predicted high-risk blocks (hotspots).
    """
    print("\n--- 2. Generating Proactive Inspection List ---")
    start_time = time.time()
    
    df_full_features = artifacts['df_features'].copy()
    best_model = artifacts['model']
    scaler = artifacts['scaler']

    # 1. Isolate Features for the Latest Month
    LATEST_MONTH_YEAR = df_full_features['Month_Year'].max()
    df_live_features = df_full_features[df_full_features['Month_Year'] == LATEST_MONTH_YEAR].copy()

    # 2. Prepare X_live (Input features for prediction)
    X_live = df_live_features[FEATURES]

    # 3. Scale the features using the established training scaler
    X_live_scaled = X_live.copy()
    X_live_scaled[COUNT_FEATURES] = scaler.transform(X_live[COUNT_FEATURES])
    
    # To avoid the scikit-learn warning, convert X_live to a numpy array BEFORE prediction
    X_live_final = X_live_scaled[FEATURES].values

    # 4. Predict the probabilities and classes
    live_pred_proba = best_model.predict_proba(X_live_final)[:, 1]
    live_pred = best_model.predict(X_live_final)

    # 5. Create the Live Prediction DataFrame
    df_live_predictions = df_live_features.copy()
    df_live_predictions.loc[:, 'Predicted_Risk'] = live_pred
    df_live_predictions.loc[:, 'High_Risk_Prob'] = live_pred_proba

    # Extract Lat/Lon for mapping purposes (Safe due to the fix in load_deployment_artifacts)
    df_live_predictions.loc[:, 'Lat'] = df_live_predictions['Block_ID'].apply(lambda x: float(x.split('_')[0]))
    df_live_predictions.loc[:, 'Lon'] = df_live_predictions['Block_ID'].apply(lambda x: float(x.split('_')[1]))
    
    # Determine the prediction month string
    # Assuming 'YYYY-MM to YYYY-MM' format for Month_Year
    latest_month_start = LATEST_MONTH_YEAR.split(' to ')[0]
    latest_month_dt = pd.to_datetime(latest_month_start)
    
    # The target is the next month's risk
    prediction_month_dt = latest_month_dt + pd.DateOffset(months=1)
    prediction_month_str = prediction_month_dt.strftime('%B %Y').upper()

    # Filter for blocks predicted as high risk for immediate proactive action
    df_hotspots = df_live_predictions[df_live_predictions['Predicted_Risk'] == 1].copy()

    end_time = time.time()
    print(f"Prediction Complete. Time taken: {end_time - start_time:.2f} seconds.")
    print(f"Prediction for {prediction_month_str}: **{len(df_hotspots)}** high-risk blocks identified.")
    
    return df_hotspots, prediction_month_str


def generate_deployment_map(df_hotspots, prediction_month_str):
    """Generates the interactive HeatMap deployment map with top markers."""
    print("\n--- 3. Generating Live Deployment Map ---")
    
    # Determine the center point
    if not df_hotspots.empty:
        # Use mean of all high-risk spots
        map_center_live = [df_hotspots['Lat'].mean(), df_hotspots['Lon'].mean()]
    else:
        # Default NYC center (as this is likely NYC-based data)
        map_center_live = [40.730610, -73.935242]

    # Create the deployment map
    m_live = folium.Map(location=map_center_live, zoom_start=11, tiles="cartodbpositron")
    

    # Add HeatMap layer to show density of risk (using High_Risk_Prob as intensity)
    if not df_hotspots.empty:
        # Note: HeatMap expects data as [Lat, Lon, Intensity]
        HeatMap(
            data=df_hotspots[['Lat', 'Lon', 'High_Risk_Prob']].values,
            radius=15,
            min_opacity=0.4
        ).add_to(m_live)

    # --- Define Top Priority Targets (Markers) ---
    df_top_hotspots = df_hotspots.sort_values(by='High_Risk_Prob', ascending=False).head(TOP_N_MARKERS)

    # Helper function to create the detailed popup (Includes Neglect Gate features)
    def create_popup(row):
        return f"""
        <h4>{prediction_month_str} RISK (Prob: {row['High_Risk_Prob']:.2f})</h4>
        ---
        <b>Block ID:</b> {row['Block_ID']}
        <b>Neglect Signal (Lag 1 Dumping):</b> {row['lag_1_dumping']:.0f}
        <b>Chronic Issue (Lag 3 Avg Rat):</b> {row['lag_3_avg_rat']:.2f}
        """

    # Add all Top N Markers (Simple Orange Circle for visibility)
    for index, row in df_top_hotspots.iterrows():
        folium.CircleMarker(
            location=[row['Lat'], row['Lon']],
            radius=5,  # Smaller radius
            color='darkorange',
            fill=True,
            fill_color='orange',
            fill_opacity=0.9,
            popup=folium.Popup(create_popup(row), max_width=300)
        ).add_to(m_live)

    # --- Add Custom HTML Legend (Bottom Right Position) ---
    legend_html = f'''
        <div style="position: fixed; 
                    bottom: 50px; right: 50px; width: 280px; height: 180px; 
                    border:2px solid grey; z-index:9999; font-size:14px;
                    background-color: white; opacity: 0.9; padding: 5px;">
            &nbsp; <b>{prediction_month_str} Inspection Targets</b> <br>
            <i style="background:orange; color:orange; border-radius:50%; width:10px; height:10px; display:inline-block; margin-left: 10px;"></i>
            <span style="margin-left: 5px;">Top {len(df_top_hotspots)} Priority Blocks</span> <br>
            <br>
            &nbsp; HeatMap Risk Density<br>
            <div style="margin: 5px 10px; width: 80%; height: 20px; 
                        background: linear-gradient(to right, blue, cyan, lime, yellow, red);">
            </div>
            <table style="width: 80%; margin: 0 10px; text-align: center;">
              <tr>
                <td style="width: 50%;">Low Risk</td>
                <td style="width: 50%;">High Risk</td>
              </tr>
            </table>
        </div>
        '''

    m_live.get_root().html.add_child(folium.Element(legend_html))

    # FIX: Save map to the REPORT_DIR
    map_filename = os.path.join(REPORT_DIR, f'risk_map_deployment.html')
    m_live.save(map_filename)
    
    print(f"Saved live inspection map to {map_filename}")


def export_inspection_list(df_hotspots, prediction_month_str):
    """Exports the top risk list as a CSV for reporting."""
    
    if df_hotspots.empty:
        print("No high-risk blocks identified to export.")
        return
        
    # Filter only the top N for the official list
    df_export = df_hotspots.sort_values(by='High_Risk_Prob', ascending=False).head(TOP_N_MARKERS).copy()
    
    # Select key columns for the final inspection report
    df_export = df_export[['Block_ID', 'Lat', 'Lon', 'High_Risk_Prob', 'lag_1_rat', 'lag_1_dumping', 'lag_3_avg_rat']].copy()
    df_export.rename(columns={
        'High_Risk_Prob': 'Risk Score (P=High)',
        'lag_1_rat': 'Prior Rat Count (1 Month)',
        'lag_1_dumping': 'Prior Dumping Count (1 Month)',
        'lag_3_avg_rat': 'Prior Rat Avg (3 Month)'
    }, inplace=True)
    
    # Add Rank
    df_export.insert(0, 'Rank', range(1, len(df_export) + 1))
    
    # FIX: Save CSV to the REPORT_DIR
    csv_filename = os.path.join(REPORT_DIR, f'inspection_list_{prediction_month_str.replace(" ", "_")}.csv')
    df_export.to_csv(csv_filename, index=False)
    
    print(f"Saved inspection list CSV to {csv_filename}")


if __name__ == '__main__':
    start_total = time.time()
    
    # 1. Load Deployment Artifacts
    artifacts = load_deployment_artifacts()
    if artifacts is None:
        exit()
        
    # 2. Generate Predictions and Identify Hotspots
    df_hotspots, prediction_month_str = generate_proactive_predictions(artifacts)
    
    # 3. Generate Deployment Map and Export List
    if not df_hotspots.empty:
        generate_deployment_map(df_hotspots, prediction_month_str)
        export_inspection_list(df_hotspots, prediction_month_str)
    else:
        print("\nSkipping map and CSV export as no high-risk blocks were identified.")
    
    end_total = time.time()
    print(f"\n--- Deployment Prediction Complete ---")
    print(f"Total prediction time: {end_total - start_total:.2f} seconds.")

    # Restore warning settings to default after this script runs
    warnings.filterwarnings("default")